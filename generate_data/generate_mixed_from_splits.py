"""
ä»è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆå¯è§£è°ƒï¼‰åˆ‡ç‰‡ç”Ÿæˆæ··åˆä¿¡å·å¯¹ï¼š
- ä¸¤ä¸¤åŠ å’Œï¼Œéšæœºå¹…åº¦æ¯”ï¼ˆç»™ç¬¬äºŒè·¯ä½œç”¨[0.2,0.9]ï¼‰
- å¯é€‰ï¼šè¯„ä¼°SNRå¹¶æ·»åŠ å™ªå£°è‡³ç›®æ ‡SNRï¼ˆä»…å¯¹å¯è§£è°ƒè®­ç»ƒæ•°æ®ï¼‰
- é…å¯¹ç­–ç•¥ï¼š
  * å…è®¸åˆ‡ç‰‡å¤ç”¨ï¼Œä½†å°½é‡å‡åŒ€ä½¿ç”¨ä¸åŒåˆ‡ç‰‡
  * ç¡®ä¿æ¯ä¸ªåˆ‡ç‰‡å¯¹(i,j)åªç”¨ä¸€æ¬¡ï¼ˆä¸é‡å¤ç›¸åŒçš„ä¸¤ä¸ªåˆ‡ç‰‡ç»„åˆï¼‰
  * åªä½¿ç”¨è®­ç»ƒé›†å†…éƒ¨å’Œæµ‹è¯•é›†å†…éƒ¨é…å¯¹ï¼Œé¿å…æ•°æ®æ³„éœ²
- åˆ†åˆ«ä¿å­˜ï¼šè®­ç»ƒé›†é…å¯¹ä¿å­˜åˆ° output_dir/train/ï¼Œæµ‹è¯•é›†é…å¯¹ä¿å­˜åˆ° output_dir/test/
- åˆ†shardå­˜å‚¨ï¼Œæ¯ä¸ªshardé»˜è®¤10kç»„ä¿¡å·å¯¹
- å­˜å‚¨æ ¼å¼å¯¹é½generate_sim_dataset.py

ä½¿ç”¨æ–¹å¼ï¼špython generate_mixed_from_splits.py [--config configs/base_config.yaml]
é…ç½®ä» YAML æ–‡ä»¶çš„ data_generation.generate_mixed éƒ¨åˆ†è¯»å–
"""

import json
import numpy as np
import torch
from pathlib import Path
from datetime import datetime
import argparse
import yaml
import random
import time
import sys
from multiprocessing import Pool, cpu_count
sys.path.append(str(Path(__file__).parent))


# ============= é…ç½®åŠ è½½å‡½æ•° =============
def load_config_from_yaml(config_path):
    """ä» YAML æ–‡ä»¶åŠ è½½é…ç½®"""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    if 'data_generation' not in config:
        raise ValueError("YAML é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘ 'data_generation' éƒ¨åˆ†")
    
    raw_data_cfg = config['data_generation'].get('raw_data', {})
    split_cfg = config['data_generation'].get('split', {})
    generate_mixed_cfg = config['data_generation'].get('generate_mixed', {})
    
    # å¤„ç† amp_rangeï¼ˆå¯èƒ½æ˜¯åˆ—è¡¨ï¼Œéœ€è¦è½¬æ¢ä¸ºå…ƒç»„ï¼‰
    amp_range = generate_mixed_cfg.get('amp_range', [0.7, 0.7])
    if isinstance(amp_range, list):
        amp_range = tuple(amp_range)
    
    # å¤„ç† amp_list
    amp_list = generate_mixed_cfg.get('amp_list')
    
    # æ„å»ºé…ç½®å­—å…¸
    config_dict = {
        'modulation': generate_mixed_cfg.get('modulation', '8PSK'),
        'mode': generate_mixed_cfg.get('mode', 'both'),
        'output_dir': generate_mixed_cfg.get('output_dir', '/nas/datasets/yixin/PCMA/real_data/8psk'),
        'shard_size': generate_mixed_cfg.get('shard_size', 10000),
        'target_pairs': generate_mixed_cfg.get('target_pairs', 100000),
        'test_target_pairs': generate_mixed_cfg.get('test_target_pairs'),
        'amp_range': amp_range,
        'amp_list': amp_list,
        'sps': generate_mixed_cfg.get('sps', 8),
        'random_seed': generate_mixed_cfg.get('random_seed', 42),
        'train_demodulable': generate_mixed_cfg.get('train_demodulable', True),
        'test_demodulable': generate_mixed_cfg.get('test_demodulable', True),
        'undemodulable': generate_mixed_cfg.get('undemodulable', False),
        'add_noise_to_target_snr': generate_mixed_cfg.get('add_noise_to_target_snr', True),
        'target_snr_db': generate_mixed_cfg.get('target_snr_db', 15.0),
        'filter_type': generate_mixed_cfg.get('filter_type', 'RRC'),
        'num_workers': generate_mixed_cfg.get('num_workers'),
        'num_files_per_amp': generate_mixed_cfg.get('num_files_per_amp', 0),
        'samples_per_file': generate_mixed_cfg.get('samples_per_file', 30),
        # åˆ‡ç‰‡æ–‡ä»¶åŸºç¡€ç›®å½•ï¼ˆä» split é…ç½®ä¸­è·å–ï¼‰
        'slices_base_dir': split_cfg.get('output_dir', '/nas/datasets/yixin/PCMA/real_data'),
    }
    
    return config_dict


# ============= è¾…åŠ©å‡½æ•° =============
def find_slices_files(modulation, base_dir, mode):
    """
    æ ¹æ®è°ƒåˆ¶æ–¹å¼å’Œæ¨¡å¼è‡ªåŠ¨æŸ¥æ‰¾åˆ‡ç‰‡æ–‡ä»¶è·¯å¾„ã€‚
    
    å‚æ•°:
        modulation: è°ƒåˆ¶æ–¹å¼
        base_dir: åŸºç¡€ç›®å½•ï¼ˆsplit_from_raw_data.pyçš„è¾“å‡ºç›®å½•ï¼‰
        mode: æ¨¡å¼ ("train", "test", "both")
    
    è¿”å›:
        (train_slices_path, test_slices_path)
    """
    base_path = Path(base_dir)
    mod_dir = base_path / modulation.lower()
    
    train_path = None
    test_path = None
    
    if mode in ["train", "both"]:
        # ä¼˜å…ˆæŸ¥æ‰¾å¯è§£è°ƒçš„è®­ç»ƒé›†åˆ‡ç‰‡
        train_demod_path = mod_dir / "train_demodulable_slices.npy"
        train_path_normal = mod_dir / "train_slices.npy"
        
        if train_demod_path.exists():
            train_path = train_demod_path
        elif train_path_normal.exists():
            train_path = train_path_normal
        else:
            raise FileNotFoundError(
                f"æœªæ‰¾åˆ°è®­ç»ƒé›†åˆ‡ç‰‡æ–‡ä»¶ã€‚è¯·æ£€æŸ¥ä»¥ä¸‹è·¯å¾„ï¼š\n"
                f"  - {train_demod_path}\n"
                f"  - {train_path_normal}"
            )
    
    if mode in ["test", "both"]:
        # ä¼˜å…ˆæŸ¥æ‰¾å¯è§£è°ƒçš„æµ‹è¯•é›†åˆ‡ç‰‡
        test_demod_path = mod_dir / "test_demodulable_slices.npy"
        test_path_normal = mod_dir / "test_slices.npy"
        
        if test_demod_path.exists():
            test_path = test_demod_path
        elif test_path_normal.exists():
            test_path = test_path_normal
        else:
            raise FileNotFoundError(
                f"æœªæ‰¾åˆ°æµ‹è¯•é›†åˆ‡ç‰‡æ–‡ä»¶ã€‚è¯·æ£€æŸ¥ä»¥ä¸‹è·¯å¾„ï¼š\n"
                f"  - {test_demod_path}\n"
                f"  - {test_path_normal}"
            )
    
    return train_path, test_path



def energy_normalize_dataset(dataset):
    """
    èƒ½é‡å½’ä¸€åŒ–æ•°æ®é›†ï¼ˆä¸generate_sim_dataset.pyå¯¹é½ï¼‰
    å½’ä¸€åŒ–æ•´ä¸ªæ•°æ®é›†çš„å¹³å‡èƒ½é‡
    """
    if not dataset:
        return dataset
    
    energies = [np.mean(np.abs(e['mixsignal']) ** 2) for e in dataset]
    mean_e = np.mean(energies) if energies else 1.0
    scale = np.sqrt(mean_e)
    
    for e in dataset:
        e['mixsignal'] = e['mixsignal'] / scale
        e['rfsignal1'] = e['rfsignal1'] / scale
        e['rfsignal2'] = e['rfsignal2'] / scale
    
    return dataset


def generate_pairs_from_slices(train_slices, test_slices, target_pairs=250000, amp_range=(0.2, 0.9), seed=42):
    """
    ä»è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ‡ç‰‡ç”Ÿæˆæ··åˆä¿¡å·å¯¹ï¼ˆä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼‰
    - å…è®¸åˆ‡ç‰‡å¤ç”¨ï¼Œä½†å°½é‡å‡åŒ€ä½¿ç”¨
    - ç¡®ä¿æ¯ä¸ªåˆ‡ç‰‡å¯¹(i,j)åªç”¨ä¸€æ¬¡ï¼ˆä¸é‡å¤ç›¸åŒçš„ä¸¤ä¸ªåˆ‡ç‰‡ç»„åˆï¼‰
    - ä½¿ç”¨æŒ‰éœ€ç”Ÿæˆç­–ç•¥ï¼Œé¿å…ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰å€™é€‰é…å¯¹ï¼ˆèŠ‚çœå†…å­˜ï¼‰
    
    å‚æ•°ï¼š
      - train_slices: è®­ç»ƒé›†åˆ‡ç‰‡åˆ—è¡¨ï¼ˆå¯ä»¥ä¸ºç©ºåˆ—è¡¨ï¼Œè¡¨ç¤ºä¸ç”Ÿæˆè®­ç»ƒé›†é…å¯¹ï¼‰
      - test_slices: æµ‹è¯•é›†ï¼ˆå¯è§£è°ƒï¼‰åˆ‡ç‰‡åˆ—è¡¨ï¼ˆå¯ä»¥ä¸ºç©ºåˆ—è¡¨ï¼Œè¡¨ç¤ºä¸ç”Ÿæˆæµ‹è¯•é›†é…å¯¹ï¼‰
      - target_pairs: ç›®æ ‡ç”Ÿæˆçš„é…å¯¹æ•°é‡
      - amp_range: å¹…åº¦æ¯”èŒƒå›´ï¼ˆä½œç”¨åœ¨ç¬¬äºŒè·¯ï¼‰
      - seed: éšæœºç§å­
    
    è¿”å›ï¼š
      - pairs: åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (source1, idx1, source2, idx2, amp_ratio)
    """
    np.random.seed(seed)
    random.seed(seed)
    
    pairs = []
    used_pairs = set()  # è®°å½•å·²ä½¿ç”¨çš„åˆ‡ç‰‡å¯¹ï¼Œé¿å…é‡å¤
    
    # è®¡ç®—æ€»å€™é€‰é…å¯¹æ•°é‡ï¼ˆç”¨äºæ˜¾ç¤ºï¼Œä¸å®é™…ç”Ÿæˆï¼‰
    train_candidates = len(train_slices) * (len(train_slices) - 1) if len(train_slices) > 0 else 0
    test_candidates = len(test_slices) * (len(test_slices) - 1) if len(test_slices) > 0 else 0
    total_candidates = train_candidates + test_candidates
    
    print(f"  æ€»å€™é€‰é…å¯¹: {total_candidates:,} (è®­ç»ƒé›†: {train_candidates:,}, æµ‹è¯•é›†: {test_candidates:,})")
    print(f"  ç›®æ ‡é…å¯¹: {target_pairs:,}")
    print(f"  ä½¿ç”¨æŒ‰éœ€ç”Ÿæˆç­–ç•¥ï¼ˆèŠ‚çœå†…å­˜ï¼‰...")
    
    # æŒ‰éœ€ç”Ÿæˆé…å¯¹ï¼Œé¿å…ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰å€™é€‰
    pair_count = 0
    max_attempts = min(target_pairs * 2, total_candidates)  # æœ€å¤šå°è¯•æ¬¡æ•°
    attempts = 0
    last_print_count = 0
    last_print_time = None
    start_time = time.time()
    
    # åˆ›å»ºç´¢å¼•åˆ—è¡¨ç”¨äºéšæœºé‡‡æ ·
    train_indices = list(range(len(train_slices))) if len(train_slices) > 0 else []
    test_indices = list(range(len(test_slices))) if len(test_slices) > 0 else []
    
    print(f"  å¼€å§‹ç”Ÿæˆé…å¯¹ï¼ˆæœ€å¤šå°è¯• {max_attempts:,} æ¬¡ï¼‰...")
    
    while pair_count < target_pairs and attempts < max_attempts:
        attempts += 1
        
        # éšæœºé€‰æ‹©æ˜¯è®­ç»ƒé›†è¿˜æ˜¯æµ‹è¯•é›†é…å¯¹
        if len(train_indices) > 0 and len(test_indices) > 0:
            use_train = random.random() < (len(train_indices) / (len(train_indices) + len(test_indices)))
        elif len(train_indices) > 0:
            use_train = True
        elif len(test_indices) > 0:
            use_train = False
        else:
            break
        
        if use_train:
            # è®­ç»ƒé›†å†…éƒ¨é…å¯¹
            idx1 = random.choice(train_indices)
            idx2 = random.choice(train_indices)
            if idx1 == idx2:
                continue
            source1, source2 = 'train', 'train'
        else:
            # æµ‹è¯•é›†å†…éƒ¨é…å¯¹
            idx1 = random.choice(test_indices)
            idx2 = random.choice(test_indices)
            if idx1 == idx2:
                continue
            source1, source2 = 'test', 'test'
        
        # æ£€æŸ¥è¿™ä¸ªé…å¯¹æ˜¯å¦å·²ä½¿ç”¨ï¼ˆé¿å…é‡å¤ç›¸åŒçš„ä¸¤ä¸ªåˆ‡ç‰‡ç»„åˆï¼‰
        pair_key = (source1, idx1, source2, idx2)
        if pair_key not in used_pairs:
            amp_ratio = np.random.uniform(*amp_range)
            pairs.append((source1, idx1, source2, idx2, amp_ratio))
            used_pairs.add(pair_key)
            pair_count += 1
            
            # æ¯ç”Ÿæˆ1000ä¸ªé…å¯¹æˆ–æ¯5ç§’è¾“å‡ºä¸€æ¬¡è¿›å±•
            current_time = time.time()
            should_print = (pair_count - last_print_count >= 1000) or \
                          (last_print_time is None or current_time - last_print_time >= 5)
            
            if should_print:
                elapsed = current_time - start_time
                rate = pair_count / elapsed if elapsed > 0 else 0
                success_rate = (pair_count / attempts * 100) if attempts > 0 else 0
                remaining = target_pairs - pair_count
                eta = remaining / rate if rate > 0 else 0
                
                print(f"    å·²ç”Ÿæˆ {pair_count:,}/{target_pairs:,} ä¸ªé…å¯¹ "
                      f"(å°è¯•: {attempts:,}, æˆåŠŸç‡: {success_rate:.1f}%, "
                      f"é€Ÿåº¦: {rate:.0f} å¯¹/ç§’, é¢„è®¡å‰©ä½™: {eta:.0f}ç§’)")
                
                last_print_count = pair_count
                last_print_time = current_time
    
    elapsed_total = time.time() - start_time
    
    print(f"\n  é…å¯¹ç”Ÿæˆå®Œæˆ:")
    print(f"    ç”Ÿæˆé…å¯¹: {pair_count:,}/{target_pairs:,}")
    print(f"    æ€»å°è¯•æ¬¡æ•°: {attempts:,}")
    if attempts > 0:
        print(f"    æˆåŠŸç‡: {pair_count/attempts*100:.2f}%")
    print(f"    æ€»è€—æ—¶: {elapsed_total:.1f} ç§’")
    if elapsed_total > 0:
        print(f"    å¹³å‡é€Ÿåº¦: {pair_count/elapsed_total:.0f} å¯¹/ç§’")
    
    if pair_count < target_pairs:
        print(f"  è­¦å‘Š: åªèƒ½ç”Ÿæˆ {pair_count:,} ä¸ªä¸é‡å¤é…å¯¹ï¼Œå°‘äºç›®æ ‡ {target_pairs:,}")
        print(f"  åŸå› : å°è¯•äº† {attempts:,} æ¬¡ï¼Œå¯èƒ½å€™é€‰é…å¯¹å·²ç”¨å®Œæˆ–éšæœºé‡‡æ ·æ•ˆç‡è¾ƒä½")
        print(f"  å»ºè®®: å¦‚æœå€™é€‰é…å¯¹å……è¶³ï¼Œå¯ä»¥å¢åŠ  max_attempts æˆ–ä½¿ç”¨ä¸åŒçš„éšæœºç­–ç•¥")
    
    # ç»Ÿè®¡æ¯ä¸ªåˆ‡ç‰‡çš„ä½¿ç”¨æ¬¡æ•°ï¼ˆä¸¥æ ¼éš”ç¦»æ£€æŸ¥ï¼‰
    train_usage = {}
    test_usage = {}
    has_train = len(train_slices) > 0
    has_test = len(test_slices) > 0
    
    for source1, idx1, source2, idx2, _ in pairs:
        # æ•°æ®æ³„éœ²æ£€æŸ¥ï¼šå¦‚æœåªæœ‰è®­ç»ƒé›†ï¼Œä¸åº”è¯¥æœ‰æµ‹è¯•é›†é…å¯¹
        if not has_test and (source1 == 'test' or source2 == 'test'):
            raise RuntimeError(f"æ•°æ®æ³„éœ²æ£€æµ‹å¤±è´¥: åœ¨ä»…è®­ç»ƒé›†æ¨¡å¼ä¸‹å‘ç°æµ‹è¯•é›†é…å¯¹ï¼({source1}, {source2})")
        # æ•°æ®æ³„éœ²æ£€æŸ¥ï¼šå¦‚æœåªæœ‰æµ‹è¯•é›†ï¼Œä¸åº”è¯¥æœ‰è®­ç»ƒé›†é…å¯¹
        if not has_train and (source1 == 'train' or source2 == 'train'):
            raise RuntimeError(f"æ•°æ®æ³„éœ²æ£€æµ‹å¤±è´¥: åœ¨ä»…æµ‹è¯•é›†æ¨¡å¼ä¸‹å‘ç°è®­ç»ƒé›†é…å¯¹ï¼({source1}, {source2})")
        
        if source1 == 'train':
            train_usage[idx1] = train_usage.get(idx1, 0) + 1
        else:
            test_usage[idx1] = test_usage.get(idx1, 0) + 1
        if source2 == 'train':
            train_usage[idx2] = train_usage.get(idx2, 0) + 1
        else:
            test_usage[idx2] = test_usage.get(idx2, 0) + 1
    
    if train_usage:
        avg_train_usage = np.mean(list(train_usage.values()))
        max_train_usage = max(train_usage.values())
        print(f"  è®­ç»ƒé›†åˆ‡ç‰‡ä½¿ç”¨ç»Ÿè®¡: å¹³å‡ {avg_train_usage:.2f} æ¬¡ï¼Œæœ€å¤š {max_train_usage} æ¬¡")
    if test_usage:
        avg_test_usage = np.mean(list(test_usage.values()))
        max_test_usage = max(test_usage.values())
        print(f"  æµ‹è¯•é›†åˆ‡ç‰‡ä½¿ç”¨ç»Ÿè®¡: å¹³å‡ {avg_test_usage:.2f} æ¬¡ï¼Œæœ€å¤š {max_test_usage} æ¬¡")
    
    return pairs


def _create_entry_worker(args_tuple):
    """
    å¤šè¿›ç¨‹å·¥ä½œå‡½æ•°ï¼šåˆ›å»ºæ•°æ®æ¡ç›®
    å‚æ•°è¢«æ‰“åŒ…æˆå…ƒç»„ä»¥æ”¯æŒmultiprocessing
    """
    (sig1, sig2, amp_ratio, modulation, sps, source1, idx1, source2, idx2,
     add_noise_to_target_snr, target_snr_db, filter_type, seed) = args_tuple
    
    # ç¡®ä¿è¾“å…¥æ˜¯numpyæ•°ç»„ï¼ˆmultiprocessingä¼ é€’æ—¶å¯èƒ½éœ€è¦ï¼‰
    sig1 = np.asarray(sig1, dtype=np.complex128)
    sig2 = np.asarray(sig2, dtype=np.complex128)
    
    return create_entry(sig1, sig2, amp_ratio, modulation, sps, source1, idx1, source2, idx2,
                       add_noise_to_target_snr, target_snr_db, filter_type, seed)


def create_entry(sig1, sig2, amp_ratio, modulation, sps=8, source1='train', idx1=0, source2='train', idx2=0,
                 add_noise_to_target_snr=False, target_snr_db=15.0, filter_type="RRC", seed=None):
    """
    åˆ›å»ºæ•°æ®æ¡ç›®ï¼ˆä¸generate_sim_dataset.pyæ ¼å¼å¯¹é½ï¼‰
    
    å‚æ•°ï¼š
      - sig1: ç¬¬ä¸€è·¯ä¿¡å·
      - sig2: ç¬¬äºŒè·¯ä¿¡å·
      - amp_ratio: å¹…åº¦æ¯”ï¼ˆä½œç”¨åœ¨ç¬¬äºŒè·¯ï¼‰
      - modulation: è°ƒåˆ¶æ–¹å¼
      - sps: æ¯ç¬¦å·é‡‡æ ·æ•°
      - source1, idx1: ç¬¬ä¸€è·¯ä¿¡å·çš„æ¥æºå’Œç´¢å¼•
      - source2, idx2: ç¬¬äºŒè·¯ä¿¡å·çš„æ¥æºå’Œç´¢å¼•
      - add_noise_to_target_snr: æ˜¯å¦å¯¹æ··åˆä¿¡å·æ·»åŠ å™ªå£°
      - target_snr_db: ç›®æ ‡SNRï¼ˆå‡è®¾ä¿¡å·1å’Œä¿¡å·2æ— å™ªï¼Œç›´æ¥å¯¹åˆè·¯ä¿¡å·åŠ å™ªè‡³ç›®æ ‡SNRï¼‰
      - filter_type: æ»¤æ³¢å™¨ç±»å‹ï¼ˆå·²ä¸ä½¿ç”¨ï¼Œä¿ç•™ä»¥å…¼å®¹æ—§ä»£ç ï¼‰
      - seed: éšæœºç§å­ï¼ˆç”¨äºå™ªå£°ç”Ÿæˆï¼‰
    
    è¿”å›ï¼š
      - entry: æ•°æ®æ¡ç›®å­—å…¸
    """
    # å¯¹é½é•¿åº¦
    min_len = min(len(sig1), len(sig2))
    sig1_aligned = sig1[:min_len]
    sig2_aligned = sig2[:min_len]
    
    # åº”ç”¨å¹…åº¦æ¯”åˆ°ç¬¬äºŒè·¯
    sig2_scaled = sig2_aligned * amp_ratio
    
    # æ··åˆä¿¡å·
    mixsignal = sig1_aligned + sig2_scaled
    
    # å™ªå£°æ·»åŠ é€»è¾‘ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š
    # 1. å‡è®¾ä¿¡å·1å’Œä¿¡å·2éƒ½æ˜¯æ— å™ªçš„
    # 2. ç›´æ¥å¯¹åˆè·¯ä¿¡å·åŠ å™ªä½¿å…¶è¾¾åˆ°ç›®æ ‡SNR
    # 3. ä¸å†è¯„ä¼°åŸå§‹ä¿¡å™ªæ¯”ï¼Œä¸å†ä¸¢å¼ƒä½SNRæ•°æ®
    actual_snr_db = 999.0  # é»˜è®¤æ— å™ªå£°ï¼ˆä¸åŠ å™ªæ—¶ï¼‰
    
    if add_noise_to_target_snr:
        # è®¡ç®—æ··åˆä¿¡å·çš„ä¿¡å·åŠŸç‡ï¼ˆå‡è®¾æ— å™ªå£°ï¼‰
        signal_power = np.mean(np.abs(mixsignal) ** 2)
        
        # ç›®æ ‡SNRè½¬æ¢ä¸ºçº¿æ€§å€¼
        target_snr_linear = 10 ** (target_snr_db / 10)
        
        # è®¡ç®—ç›®æ ‡å™ªå£°åŠŸç‡ï¼šSNR = ä¿¡å·åŠŸç‡ / å™ªå£°åŠŸç‡
        # å› æ­¤ï¼šå™ªå£°åŠŸç‡ = ä¿¡å·åŠŸç‡ / SNR
        target_noise_power = signal_power / target_snr_linear
        
        # ç”ŸæˆAWGNå™ªå£°
        if seed is not None:
            np.random.seed(seed)
        
        # å™ªå£°æ ‡å‡†å·®ï¼ˆå¤å™ªå£°ï¼Œå®éƒ¨å’Œè™šéƒ¨å„å ä¸€åŠåŠŸç‡ï¼‰
        noise_std = np.sqrt(target_noise_power / 2)
        noise = noise_std * (np.random.randn(len(mixsignal)) + 1j * np.random.randn(len(mixsignal)))
        
        # æ·»åŠ å™ªå£°åˆ°æ··åˆä¿¡å·
        mixsignal = mixsignal + noise
        actual_snr_db = target_snr_db
    
    # åˆ›å»ºæ¡ç›®ï¼ˆä¸generate_sim_dataset.pyæ ¼å¼å¯¹é½ï¼‰
    # paramsæ ¼å¼: (snr_db, amplitude_ratio, sps, f_off1_str, f_off2_str, phi1_str, phi2_str, delay1_str, delay2_str, mod1_str, mod2_str, ...)
    entry = {
        'mixsignal': mixsignal,
        'rfsignal1': sig1_aligned,
        'rfsignal2': sig2_scaled,  # æ³¨æ„ï¼šè¿™é‡Œä¿å­˜çš„æ˜¯ç¼©æ”¾åçš„ç¬¬äºŒè·¯ä¿¡å·
        'params': (
            float(actual_snr_db),  # snr_db (å®é™…SNR)
            float(amp_ratio),  # amplitude_ratio
            int(sps),  # sps
            'f_off1=0.00Hz',  # f_off1 (æ— é¢‘åï¼Œè®¾ä¸º0)
            'f_off2=0.00Hz',  # f_off2 (æ— é¢‘åï¼Œè®¾ä¸º0)
            'phi1=0.0000rad',  # phi1 (æ— ç›¸åï¼Œè®¾ä¸º0)
            'phi2=0.0000rad',  # phi2 (æ— ç›¸åï¼Œè®¾ä¸º0)
            'delay1_samp=0',  # delay1_samp (æ— æ—¶å»¶å·®ï¼Œè®¾ä¸º0)
            'delay2_samp=0',  # delay2_samp (æ— æ—¶å»¶å·®ï¼Œè®¾ä¸º0)
            f'mod1={modulation}',  # mod1
            f'mod2={modulation}',  # mod2
            f'source1={source1}_idx{idx1}',  # ç¬¬ä¸€è·¯æ¥æº
            f'source2={source2}_idx{idx2}',  # ç¬¬äºŒè·¯æ¥æº
        ),
        'bits1': np.array([], dtype=np.int8),  # å®é‡‡æ•°æ®æ²¡æœ‰æ¯”ç‰¹ä¿¡æ¯
        'bits2': np.array([], dtype=np.int8),
        'origin_len': 1
    }
    
    # ä¸å†ä¿å­˜SNRç»Ÿè®¡ä¿¡æ¯ï¼ˆå› ä¸ºä¸å†è¯„ä¼°åŸå§‹SNRï¼‰
    
    return entry


def process_test_pairs_with_amp(test_pairs, test_slices, config, amp_ratio):
    """
    å¤„ç†æµ‹è¯•é›†é…å¯¹ï¼ˆä½¿ç”¨æŒ‡å®šçš„å›ºå®šå¹…åº¦æ¯”ï¼‰
    æ”¯æŒç”Ÿæˆå¤šä¸ªæ–‡ä»¶ï¼ˆæ¯ä¸ªæ–‡ä»¶30ä¸ªæ ·æœ¬ï¼‰ï¼ŒåŒæ—¶ç”Ÿæˆæ— å™ªå’ŒåŠ å™ªä¸¤ç§ç‰ˆæœ¬
    è¿”å›ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    test_saved_paths = []
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯test1_2æˆ–test2_2æ¨¡å¼ï¼ˆéœ€è¦ç”Ÿæˆå¤šä¸ªæ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶30ä¸ªæ ·æœ¬ï¼‰
    is_test12_or_test22 = config.get('num_files_per_amp', 0) > 0
    if is_test12_or_test22:
        num_files = config['num_files_per_amp']
        samples_per_file = config.get('samples_per_file', 30)
        print(f"\nå¤„ç†æµ‹è¯•é›†é…å¯¹ï¼ˆå¹…åº¦æ¯”={amp_ratio}ï¼Œæ¯ä¸ªæ–‡ä»¶ {samples_per_file} ç»„ï¼Œç”Ÿæˆ {num_files} ä¸ªæ–‡ä»¶ï¼‰...")
    else:
        num_files = 1
        samples_per_file = config['shard_size']
        print(f"\nå¤„ç†æµ‹è¯•é›†é…å¯¹ï¼ˆå¹…åº¦æ¯”={amp_ratio}ï¼Œæ¯ä¸ªshard {config['shard_size']} ç»„ï¼‰...")
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦æ·»åŠ å™ªå£°ï¼ˆä»…å¯¹å¯è§£è°ƒæµ‹è¯•æ•°æ®ï¼Œä¸”æ˜ç¡®æŒ‡å®šäº†add_noise_to_target_snrï¼‰
    add_noise_test = config.get('add_noise_to_target_snr', False) and config.get('test_demodulable', False)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    nonoise_output_dir = Path(config['output_dir']) / "nonoise"
    noise_output_dir = Path(config['output_dir']) / "test" / "demodulable" if config.get('test_demodulable', False) else Path(config['output_dir']) / "test"
    
    if is_test12_or_test22:
        # test1_2 æˆ– test2_2 æ¨¡å¼ï¼šç”Ÿæˆå¤šä¸ªæ–‡ä»¶ï¼ŒåŒæ—¶ç”Ÿæˆæ— å™ªå’ŒåŠ å™ªç‰ˆæœ¬
        print(f"  å°†åŒæ—¶ç”Ÿæˆæ— å™ªç‰ˆæœ¬ï¼ˆä¿å­˜åˆ° nonoise/ï¼‰å’ŒåŠ å™ªç‰ˆæœ¬ï¼ˆä¿å­˜åˆ° test/demodulable/ï¼‰")
        if add_noise_test:
            print(f"  åŠ å™ªç‰ˆæœ¬ï¼šå°†è¯„ä¼°ä¸¤è·¯æºä¿¡å·å’Œæ··åˆä¿¡å·SNRï¼Œå¦‚æœæ··åˆä¿¡å·SNR>{config['target_snr_db']}dBï¼Œåˆ™æ·»åŠ å™ªå£°è‡³{config['target_snr_db']}dB")
            print(f"  å¦‚æœæ··åˆä¿¡å·SNR<{config['target_snr_db']}dBï¼Œåˆ™ä¸¢å¼ƒè¯¥é…å¯¹")
            print(f"  æ»¤æ³¢å™¨ç±»å‹ï¼š{config.get('filter_type', 'RRC')}")
        nonoise_output_dir.mkdir(parents=True, exist_ok=True)
        if add_noise_test:
            noise_output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¸ºæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆä¸åŒçš„é…å¯¹ï¼ˆä½¿ç”¨ä¸åŒçš„éšæœºç§å­ï¼‰
        for file_idx in range(num_files):
            file_seed = config['random_seed'] + file_idx * 1000 + int(amp_ratio * 1000)  # åŸºäºæ–‡ä»¶ç´¢å¼•å’Œå¹…åº¦æ¯”çš„ç§å­
            
            print(f"\n{'='*60}")
            print(f"ç”Ÿæˆæ–‡ä»¶ {file_idx + 1}/{num_files} (å¹…åº¦æ¯”={amp_ratio}, seed={file_seed})")
            print(f"{'='*60}")
            
            # ä¸ºå½“å‰æ–‡ä»¶ç”Ÿæˆé…å¯¹ï¼ˆæ¯ä¸ªæ–‡ä»¶éœ€è¦ samples_per_file ä¸ªé…å¯¹ï¼Œä½†åŠ å™ªç‰ˆæœ¬å¯èƒ½ä¼šä¸¢å¼ƒä¸€äº›ï¼Œæ‰€ä»¥ç”Ÿæˆæ›´å¤šï¼‰
            target_pairs_per_file = samples_per_file * 2 if add_noise_test else samples_per_file  # åŠ å™ªç‰ˆæœ¬å¯èƒ½éœ€è¦æ›´å¤šé…å¯¹
            
            file_pairs_list = generate_pairs_from_slices(
                [], test_slices,
                target_pairs=target_pairs_per_file,
                amp_range=(amp_ratio, amp_ratio),
                seed=file_seed
            )
            file_pairs = [(s1, i1, s2, i2, amp) for s1, i1, s2, i2, amp in file_pairs_list 
                         if s1 == 'test' and s2 == 'test']
            
            # å¤„ç†æ— å™ªç‰ˆæœ¬
            print(f"  å¤„ç†æ— å™ªç‰ˆæœ¬...")
            nonoise_entries = process_file_pairs(file_pairs, test_slices, config, amp_ratio, 
                                                add_noise=False, file_idx=file_idx, file_seed=file_seed)
            
            # åªå–å‰ samples_per_file ä¸ª
            if len(nonoise_entries) > samples_per_file:
                nonoise_entries = nonoise_entries[:samples_per_file]
            
            if len(nonoise_entries) > 0:
                save_path = save_shard(nonoise_entries, file_idx, nonoise_output_dir,
                                      config['modulation'], num_files, (amp_ratio, amp_ratio),
                                      seed=file_seed, is_nonoise=True)
                if save_path:
                    test_saved_paths.append(save_path)
                print(f"  âœ… æ— å™ªç‰ˆæœ¬å·²ä¿å­˜: {save_path} (æ ·æœ¬æ•°: {len(nonoise_entries)})")
            
            # å¤„ç†åŠ å™ªç‰ˆæœ¬ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if add_noise_test:
                print(f"  å¤„ç†åŠ å™ªç‰ˆæœ¬...")
                noise_entries = process_file_pairs(file_pairs, test_slices, config, amp_ratio,
                                                   add_noise=True, file_idx=file_idx, file_seed=file_seed)
                
                # åªå–å‰ samples_per_file ä¸ª
                if len(noise_entries) > samples_per_file:
                    noise_entries = noise_entries[:samples_per_file]
                
                if len(noise_entries) > 0:
                    save_path = save_shard(noise_entries, file_idx, noise_output_dir,
                                          config['modulation'], num_files, (amp_ratio, amp_ratio),
                                          seed=file_seed, is_nonoise=False)
                    if save_path:
                        test_saved_paths.append(save_path)
                    print(f"  âœ… åŠ å™ªç‰ˆæœ¬å·²ä¿å­˜: {save_path} (æ ·æœ¬æ•°: {len(noise_entries)})")
    else:
        # åŸæœ‰é€»è¾‘ï¼šå•ä¸ªshardæ–‡ä»¶
        if add_noise_test:
            print(f"  å°†è¯„ä¼°ä¸¤è·¯æºä¿¡å·å’Œæ··åˆä¿¡å·SNRï¼Œå¦‚æœæ··åˆä¿¡å·SNR>{config['target_snr_db']}dBï¼Œåˆ™æ·»åŠ å™ªå£°è‡³{config['target_snr_db']}dB")
            print(f"  å¦‚æœæ··åˆä¿¡å·SNR<{config['target_snr_db']}dBï¼Œåˆ™ä¸¢å¼ƒè¯¥é…å¯¹")
            print(f"  æ»¤æ³¢å™¨ç±»å‹ï¼š{config.get('filter_type', 'RRC')}")
        else:
            print(f"  ç›´æ¥ç»„åˆæ•°æ®ï¼Œä¸è¯„ä¼°SNRï¼Œä¸æ·»åŠ å™ªå£°")
        
        print(f"  ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†ï¼ˆ{config.get('num_workers', cpu_count())} ä¸ªå·¥ä½œè¿›ç¨‹ï¼‰...")
        
        # å‡†å¤‡å¤šè¿›ç¨‹å‚æ•°
        worker_args = []
        for pair_idx, (source1, idx1, source2, idx2, _) in enumerate(test_pairs):
            sig1 = test_slices[idx1]
            sig2 = test_slices[idx2]
            noise_seed = config['random_seed'] + pair_idx if add_noise_test else None
            worker_args.append((
                sig1, sig2, amp_ratio, config['modulation'], config['sps'],  # ä½¿ç”¨å›ºå®šçš„amp_ratio
                source1, idx1, source2, idx2,
                add_noise_test, config['target_snr_db'], config.get('filter_type', 'RRC'), noise_seed
            ))
        
        # ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†
        print(f"  å¼€å§‹å¹¶è¡Œå¤„ç† {len(test_pairs)} ä¸ªé…å¯¹...")
        start_time = time.time()
        
        test_entries = []
        discarded_count = 0  # ç»Ÿè®¡ä¸¢å¼ƒçš„æ•°é‡
        with Pool(processes=config.get('num_workers', cpu_count())) as pool:
            # ä½¿ç”¨imapä»¥ä¾¿æ˜¾ç¤ºè¿›åº¦
            results = pool.imap(_create_entry_worker, worker_args)
            
            # æ˜¾ç¤ºè¿›åº¦
            completed = 0
            total = len(worker_args)
            last_print_time = time.time()
            print_interval = 2.0  # æ¯2ç§’æ‰“å°ä¸€æ¬¡è¿›åº¦
            
            for result in results:
                completed += 1
                
                # è¿‡æ»¤æ‰Noneå€¼ï¼ˆSNRä½äºç›®æ ‡å€¼çš„è¢«ä¸¢å¼ƒï¼‰
                if result is None:
                    discarded_count += 1
                else:
                    test_entries.append(result)
                
                # å®šæœŸæ‰“å°è¿›åº¦
                current_time = time.time()
                if current_time - last_print_time >= print_interval or completed == total:
                    elapsed = current_time - start_time
                    rate = completed / elapsed if elapsed > 0 else 0
                    remaining = (total - completed) / rate if rate > 0 else 0
                    progress_pct = completed / total * 100
                    valid_count = len(test_entries)
                    print(f"    è¿›åº¦: {completed}/{total} ({progress_pct:.1f}%) | "
                          f"æœ‰æ•ˆ: {valid_count} | ä¸¢å¼ƒ: {discarded_count} | "
                          f"å·²ç”¨: {elapsed:.1f}s | é€Ÿåº¦: {rate:.1f} é…å¯¹/s | "
                          f"é¢„è®¡å‰©ä½™: {remaining:.1f}s", flush=True)
                    last_print_time = current_time
        
        elapsed_time = time.time() - start_time
        print(f"  å¹¶è¡Œå¤„ç†å®Œæˆï¼Œè€—æ—¶ {elapsed_time:.2f} ç§’ï¼ˆå¹³å‡ {elapsed_time/len(test_pairs)*1000:.2f} ms/é…å¯¹ï¼‰")
        if add_noise_test:
            print(f"  æœ‰æ•ˆé…å¯¹: {len(test_entries)} (å…¨éƒ¨ä¿ç•™)")
        else:
            print(f"  æœ‰æ•ˆé…å¯¹: {len(test_entries)} (å…¨éƒ¨ä¿ç•™ï¼Œæœªè¯„ä¼°SNR)")
        
        # ç¡®å®šè¾“å‡ºç›®å½•
        if not add_noise_test:
            test_output_dir = nonoise_output_dir
        elif config.get('test_demodulable', False):
            test_output_dir = noise_output_dir
        elif config.get('undemodulable', False):
            test_output_dir = Path(config['output_dir']) / "test" / "undemodulable"
        else:
            test_output_dir = Path(config['output_dir']) / "test"
        test_output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»Ÿè®¡SNRä¿¡æ¯
        snr_stats = []
        noise_added_count = 0
        
        test_shard_entries = []
        test_shard_idx = 0
        
        for entry in test_entries:
            test_shard_entries.append(entry)
            
            # ç»Ÿè®¡SNRä¿¡æ¯ï¼ˆä»entryä¸­æå–ï¼‰
            if add_noise_test:
                actual_snr = entry['params'][0]
                snr_stats.append(actual_snr)
                
                if actual_snr <= config['target_snr_db'] + 0.1:  # å…è®¸0.1dBè¯¯å·®
                    noise_added_count += 1
            
            # å¦‚æœè¾¾åˆ°shardå¤§å°ï¼Œä¿å­˜ï¼ˆä½¿ç”¨å½“å‰å¹…åº¦æ¯”ï¼‰
            if len(test_shard_entries) >= config['shard_size']:
                total_shards = (len(test_pairs) + config['shard_size'] - 1) // config['shard_size']
                save_path = save_shard(test_shard_entries, test_shard_idx, test_output_dir, 
                                      config['modulation'], total_shards, (amp_ratio, amp_ratio))
                if save_path:
                    test_saved_paths.append(save_path)
                test_shard_entries = []
                test_shard_idx += 1
        
        # ä¿å­˜æœ€åä¸€ä¸ªshard
        if test_shard_entries:
            total_shards = test_shard_idx + 1
            save_path = save_shard(test_shard_entries, test_shard_idx, test_output_dir, 
                                  config['modulation'], total_shards, (amp_ratio, amp_ratio))
            if save_path:
                test_saved_paths.append(save_path)
        
    # è¾“å‡ºSNRç»Ÿè®¡ä¿¡æ¯
    if add_noise_test and snr_stats:
        print(f"\n  SNRç»Ÿè®¡ä¿¡æ¯ï¼ˆå¹…åº¦æ¯”={amp_ratio}ï¼‰:")
        print(f"    æœ€ç»ˆæ··åˆä¿¡å·å¹³å‡SNR: {np.mean(snr_stats):.2f} dB")
        print(f"    æœ€ç»ˆæ··åˆä¿¡å·æœ€å°SNR: {np.min(snr_stats):.2f} dB")
        print(f"    æœ€ç»ˆæ··åˆä¿¡å·æœ€å¤§SNR: {np.max(snr_stats):.2f} dB")
        print(f"    å·²æ·»åŠ å™ªå£°æ ·æœ¬æ•°: {noise_added_count}/{len(test_pairs) if test_pairs else 0}")
    
    return test_saved_paths


def process_file_pairs(file_pairs, test_slices, config, amp_ratio, add_noise=False, file_idx=0, file_seed=42):
    """
    å¤„ç†å•ä¸ªæ–‡ä»¶çš„é…å¯¹ï¼Œè¿”å›æ•°æ®æ¡ç›®åˆ—è¡¨
    """
    print(f"    ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†ï¼ˆ{config.get('num_workers', cpu_count())} ä¸ªå·¥ä½œè¿›ç¨‹ï¼‰...")
    
    # å‡†å¤‡å¤šè¿›ç¨‹å‚æ•°
    worker_args = []
    for pair_idx, (source1, idx1, source2, idx2, _) in enumerate(file_pairs):
        sig1 = test_slices[idx1]
        sig2 = test_slices[idx2]
        noise_seed = file_seed + pair_idx if add_noise else None
        worker_args.append((
            sig1, sig2, amp_ratio, config['modulation'], config['sps'],
            source1, idx1, source2, idx2,
            add_noise, config['target_snr_db'], config.get('filter_type', 'RRC'), noise_seed
        ))
    
    # ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†
    start_time = time.time()
    
    entries = []
    discarded_count = 0
    with Pool(processes=config.get('num_workers', cpu_count())) as pool:
        results = pool.imap(_create_entry_worker, worker_args)
        
        completed = 0
        total = len(worker_args)
        
        for result in results:
            completed += 1
            
            if result is None:
                discarded_count += 1
            else:
                entries.append(result)
            
            if completed % 10 == 0 or completed == total:
                elapsed = time.time() - start_time
                rate = completed / elapsed if elapsed > 0 else 0
                valid_count = len(entries)
                print(f"      è¿›åº¦: {completed}/{total} | æœ‰æ•ˆ: {valid_count} | ä¸¢å¼ƒ: {discarded_count} | "
                      f"é€Ÿåº¦: {rate:.1f} é…å¯¹/s", flush=True)
    
    elapsed_time = time.time() - start_time
    if add_noise:
        print(f"    å¤„ç†å®Œæˆï¼Œè€—æ—¶ {elapsed_time:.2f} ç§’ï¼Œæœ‰æ•ˆ: {len(entries)}, ä¸¢å¼ƒ: {discarded_count}")
    else:
        print(f"    å¤„ç†å®Œæˆï¼Œè€—æ—¶ {elapsed_time:.2f} ç§’ï¼Œæœ‰æ•ˆ: {len(entries)}")
    
    return entries


def save_shard(entries, shard_idx, output_dir, modulation, total_shards, amp_range, seed=None, is_nonoise=False):
    """
    ä¿å­˜ä¸€ä¸ªshardï¼ˆä¸generate_sim_dataset.pyæ ¼å¼å¯¹é½ï¼‰
    
    å‚æ•°:
        seed: éšæœºç§å­ï¼ˆå¯é€‰ï¼Œç”¨äºtest1_2/test2_2æ¨¡å¼çš„æ–‡ä»¶å‘½åï¼‰
        is_nonoise: æ˜¯å¦ä¸ºæ— å™ªç‰ˆæœ¬ï¼ˆç”¨äºtest1_2/test2_2æ¨¡å¼çš„æ–‡ä»¶å‘½åï¼‰
    """
    if not entries:
        return None
    
    # ç§»é™¤ä¸´æ—¶çš„SNRç»Ÿè®¡ä¿¡æ¯ï¼ˆä¸ä¿å­˜åˆ°æ–‡ä»¶ï¼‰
    for e in entries:
        e.pop('_snr1_db', None)
        e.pop('_snr2_db', None)
        e.pop('_mix_snr_db', None)
    
    # å½’ä¸€åŒ–
    entries_norm = energy_normalize_dataset(entries)
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆç¡®ä¿æ˜¯complex128ï¼‰
    for e in entries_norm:
        e['mixsignal'] = np.asarray(e['mixsignal'], dtype=np.complex128)
        e['rfsignal1'] = np.asarray(e['rfsignal1'], dtype=np.complex128)
        e['rfsignal2'] = np.asarray(e['rfsignal2'], dtype=np.complex128)
    
    # æ„å»ºæ–‡ä»¶åï¼ˆå¯¹é½generate_sim_dataset.pyæ ¼å¼ï¼‰
    amp_min, amp_max = amp_range
    if amp_min == amp_max:
        # å›ºå®šå¹…åº¦æ¯”ï¼Œåªæ˜¾ç¤ºä¸€ä¸ªå€¼
        amp_str = f"amp{amp_min:.1f}"
    else:
        # å¹…åº¦æ¯”èŒƒå›´
        amp_str = f"amp{amp_min:.1f}to{amp_max:.1f}"
    
    # å¦‚æœæ˜¯test1_2/test2_2æ¨¡å¼ï¼Œæ–‡ä»¶ååŒ…å«ç§å­ä¿¡æ¯
    if seed is not None:
        noise_suffix = "nonoise" if is_nonoise else f"snr{int(amp_min*10)}dB"  # ç®€åŒ–å‘½å
        base_name = f"real_{modulation.lower()}_mixed_{amp_str}_{noise_suffix}_N{len(entries_norm)}_seed{seed}_c128"
    else:
        base_name = f"real_{modulation.lower()}_mixed_{amp_str}_shard{shard_idx:02d}_of{total_shards:02d}_c128"
    
    save_path = output_dir / f"{base_name}.pth"
    
    # ä¿å­˜
    torch.save(entries_norm, save_path)
    
    if seed is not None:
        print(f"ğŸ“¦ å·²ä¿å­˜æ–‡ä»¶: {save_path} ï¼ˆæ ·æœ¬æ•° {len(entries_norm)}ï¼‰")
    else:
        print(f"ğŸ“¦ å·²ä¿å­˜åˆ†ç‰‡ {shard_idx}/{total_shards}: {save_path} ï¼ˆæ ·æœ¬æ•° {len(entries_norm)}ï¼‰")
    
    return save_path


def main():
    parser = argparse.ArgumentParser(description='ä»åˆ‡ç‰‡ç”Ÿæˆæ··åˆä¿¡å·å¯¹')
    parser.add_argument('--config', type=str, default='configs/base_config.yaml',
                        help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: configs/base_config.yamlï¼‰')
    args = parser.parse_args()
    
    # ä» YAML åŠ è½½é…ç½®
    config = load_config_from_yaml(args.config)
    
    # è®¾ç½®num_workersé»˜è®¤å€¼
    if config.get('num_workers') is None:
        config['num_workers'] = min(64, cpu_count())
    else:
        config['num_workers'] = min(config['num_workers'], cpu_count())
    print(f"é…ç½®æ–‡ä»¶: {args.config}")
    print(f"ä½¿ç”¨ {config['num_workers']} ä¸ªå·¥ä½œè¿›ç¨‹ï¼ˆç³»ç»Ÿæ ¸å¿ƒæ•°: {cpu_count()}ï¼‰")
    
    # è‡ªåŠ¨æŸ¥æ‰¾åˆ‡ç‰‡æ–‡ä»¶
    train_slices_path, test_slices_path = find_slices_files(
        config['modulation'], config['slices_base_dir'], config['mode']
    )
    
    print(f"\nè°ƒåˆ¶æ–¹å¼: {config['modulation']}")
    print(f"æ¨¡å¼: {config['mode']}")
    if train_slices_path:
        print(f"è®­ç»ƒé›†åˆ‡ç‰‡æ–‡ä»¶: {train_slices_path}")
    if test_slices_path:
        print(f"æµ‹è¯•é›†åˆ‡ç‰‡æ–‡ä»¶: {test_slices_path}")
    print(f"è¾“å‡ºç›®å½•: {config['output_dir']}")
    print(f"{'='*60}\n")
    
    # ä¸¥æ ¼éš”ç¦»ï¼šåªåŠ è½½å¯¹åº”æ¨¡å¼çš„åˆ‡ç‰‡æ•°æ®
    train_slices = []
    test_slices = []
    
    if config['mode'] in ['train', 'both']:
        if train_slices_path is None:
            raise ValueError(f"ç”Ÿæˆè®­ç»ƒé›†é…å¯¹éœ€è¦è®­ç»ƒé›†åˆ‡ç‰‡æ–‡ä»¶ï¼Œä½†æœªæ‰¾åˆ°ã€‚è¯·æ£€æŸ¥ {config['slices_base_dir']}/{config['modulation'].lower()}/")
        print(f"åŠ è½½è®­ç»ƒé›†åˆ‡ç‰‡: {train_slices_path}")
        train_slices = np.load(train_slices_path, allow_pickle=True)
        train_slices = [np.asarray(s, dtype=np.complex128) for s in train_slices]
        print(f"  è®­ç»ƒé›†åˆ‡ç‰‡æ•°: {len(train_slices)}")
        if config.get('train_demodulable', False):
            print(f"  æ ‡è®°ä¸ºå¯è§£è°ƒæ•°æ®ï¼ˆå°†ä¿å­˜åˆ° train/demodulable/ ç›®å½•ï¼‰")
    else:
        train_slices = []
        print("  è®­ç»ƒé›†åˆ‡ç‰‡: æœªåŠ è½½ï¼ˆmode=testï¼Œä¸¥æ ¼éš”ç¦»ï¼‰")
    
    if config['mode'] in ['test', 'both']:
        if test_slices_path is None:
            raise ValueError(f"ç”Ÿæˆæµ‹è¯•é›†é…å¯¹éœ€è¦æµ‹è¯•é›†åˆ‡ç‰‡æ–‡ä»¶ï¼Œä½†æœªæ‰¾åˆ°ã€‚è¯·æ£€æŸ¥ {config['slices_base_dir']}/{config['modulation'].lower()}/")
        print(f"åŠ è½½æµ‹è¯•é›†åˆ‡ç‰‡: {test_slices_path}")
        test_slices = np.load(test_slices_path, allow_pickle=True)
        test_slices = [np.asarray(s, dtype=np.complex128) for s in test_slices]
        print(f"  æµ‹è¯•é›†åˆ‡ç‰‡æ•°: {len(test_slices)}")
        if config.get('test_demodulable', False):
            print(f"  æ ‡è®°ä¸ºå¯è§£è°ƒæ•°æ®ï¼ˆå°†ä¿å­˜åˆ° test/demodulable/ ç›®å½•ï¼‰")
        elif config.get('undemodulable', False):
            print(f"  æ ‡è®°ä¸ºä¸å¯è§£è°ƒæ•°æ®ï¼ˆå°†ä¿å­˜åˆ° test/undemodulable/ ç›®å½•ï¼‰")
    else:
        test_slices = []
        print("  æµ‹è¯•é›†åˆ‡ç‰‡: æœªåŠ è½½ï¼ˆmode=trainï¼Œä¸¥æ ¼éš”ç¦»ï¼‰")
    
    # ç¡®å®šæµ‹è¯•é›†ç›®æ ‡é…å¯¹æ•°é‡
    if config.get('test_target_pairs') is None:
        test_target_pairs = max(1000, config['target_pairs'] // 10)  # é»˜è®¤æ˜¯è®­ç»ƒé›†çš„10%ï¼Œè‡³å°‘1000
    else:
        test_target_pairs = config['test_target_pairs']
    
    # å¤„ç†amp_range/amp_listå‚æ•°
    amp_list = config.get('amp_list', None)
    amp_range = config.get('amp_range', (0.2, 0.9))
    
    # å¦‚æœamp_rangeæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºamp_list
    if isinstance(amp_range, list):
        amp_list = amp_range
        amp_range = None
    elif amp_list is not None:
        # å¦‚æœæŒ‡å®šäº†amp_listï¼Œå¿½ç•¥amp_range
        amp_range = None
    
    # å¤„ç†amp_listå‚æ•°ï¼šå¦‚æœæŒ‡å®šäº†å¤šä¸ªå›ºå®šå¹…åº¦æ¯”ï¼Œä¸ºæ¯ä¸ªå¹…åº¦æ¯”ç”Ÿæˆå•ç‹¬çš„æ–‡ä»¶
    if amp_list is not None and len(amp_list) > 0:
        # åªå¤„ç†æµ‹è¯•é›†ï¼ˆtest2_2æ˜¯æµ‹è¯•é›†æ•°æ®ï¼‰
        if config['mode'] in ['test', 'both']:
            print(f"\næ£€æµ‹åˆ°amp_listå‚æ•°ï¼Œå°†ä¸º {len(amp_list)} ä¸ªå¹…åº¦æ¯”ç”Ÿæˆå•ç‹¬çš„æ–‡ä»¶: {amp_list}")
            
            # ä¸ºæ¯ä¸ªå¹…åº¦æ¯”ç”Ÿæˆæ–‡ä»¶
            all_test_saved_paths = []
            for amp_idx, amp_ratio in enumerate(amp_list):
                print(f"\n{'='*60}")
                print(f"å¤„ç†å¹…åº¦æ¯” {amp_ratio} ({amp_idx+1}/{len(amp_list)})")
                print(f"{'='*60}")
                
                # ç”Ÿæˆé…å¯¹ï¼ˆä½¿ç”¨å›ºå®šå¹…åº¦æ¯”ï¼‰
                test_pairs_list = generate_pairs_from_slices(
                    [], test_slices,  # ä¸¥æ ¼éš”ç¦»ï¼šåªä¼ å…¥æµ‹è¯•é›†åˆ‡ç‰‡
                    target_pairs=test_target_pairs,
                    amp_range=(amp_ratio, amp_ratio), seed=config['random_seed'] + amp_idx
                )
                test_pairs = [(s1, i1, s2, i2, amp) for s1, i1, s2, i2, amp in test_pairs_list 
                              if s1 == 'test' and s2 == 'test']
                print(f"  æµ‹è¯•é›†é…å¯¹: {len(test_pairs)} ç»„ï¼ˆå·²éªŒè¯ï¼šå…¨éƒ¨ä¸ºæµ‹è¯•é›†å†…éƒ¨é…å¯¹ï¼‰")
                
                # å¤„ç†æµ‹è¯•é›†é…å¯¹ï¼ˆä½¿ç”¨å½“å‰å¹…åº¦æ¯”ï¼‰
                test_saved_paths = process_test_pairs_with_amp(
                    test_pairs, test_slices, config, amp_ratio
                )
                all_test_saved_paths.extend(test_saved_paths)
            
            # è®¾ç½®test_saved_pathsä¸ºæ‰€æœ‰å¹…åº¦æ¯”çš„æ–‡ä»¶
            test_saved_paths = all_test_saved_paths
            test_pairs = []  # æ¸…ç©ºï¼Œé¿å…åç»­é‡å¤å¤„ç†
        else:
            raise ValueError("amp_list å‚æ•°ä»…æ”¯æŒ mode='test' æˆ– mode='both'ï¼ˆä¸”åªå¤„ç†æµ‹è¯•é›†ï¼‰")
    
    # ç”Ÿæˆé…å¯¹ï¼ˆå¸¸è§„æ¨¡å¼ï¼Œå¦‚æœæ²¡æœ‰ä½¿ç”¨amp_listï¼‰
    train_pairs = []
    if not (amp_list is not None and len(amp_list) > 0):
        test_pairs = []
    
    if config['mode'] in ['train', 'both']:
        print(f"\nç”Ÿæˆè®­ç»ƒé›†æ··åˆä¿¡å·å¯¹...")
        assert len(test_slices) == 0 or config['mode'] == 'both', "è®­ç»ƒé›†æ¨¡å¼ä¸‹ä¸åº”æœ‰æµ‹è¯•é›†åˆ‡ç‰‡"
        train_pairs_list = generate_pairs_from_slices(
            train_slices, [],  # ä¸¥æ ¼éš”ç¦»ï¼šåªä¼ å…¥è®­ç»ƒé›†åˆ‡ç‰‡
            target_pairs=config['target_pairs'],
            amp_range=tuple(amp_range) if amp_range is not None else (0.2, 0.9), 
            seed=config['random_seed']
        )
        train_pairs = [(s1, i1, s2, i2, amp) for s1, i1, s2, i2, amp in train_pairs_list 
                       if s1 == 'train' and s2 == 'train']
        if len(train_pairs) != len(train_pairs_list):
            raise RuntimeError(f"æ•°æ®æ³„éœ²æ£€æµ‹: å‘ç°éè®­ç»ƒé›†é…å¯¹ï¼è®­ç»ƒé›†é…å¯¹ {len(train_pairs)} != æ€»é…å¯¹ {len(train_pairs_list)}")
        print(f"  è®­ç»ƒé›†é…å¯¹: {len(train_pairs)} ç»„ï¼ˆå·²éªŒè¯ï¼šå…¨éƒ¨ä¸ºè®­ç»ƒé›†å†…éƒ¨é…å¯¹ï¼‰")
    
    if config['mode'] in ['test', 'both']:
        # å¦‚æœä½¿ç”¨ num_files_per_amp æ¨¡å¼ï¼ˆtest1_2/test2_2ï¼‰ï¼Œè·³è¿‡å¸¸è§„é…å¯¹ç”Ÿæˆ
        if config.get('num_files_per_amp', 0) > 0:
            print(f"\nä½¿ç”¨ test1_2/test2_2 æ¨¡å¼ï¼šè·³è¿‡å¸¸è§„é…å¯¹ç”Ÿæˆï¼Œå°†åœ¨æ¯ä¸ªæ–‡ä»¶ä¸­å•ç‹¬ç”Ÿæˆé…å¯¹")
            test_pairs = []  # è®¾ç½®ä¸ºç©ºï¼Œprocess_test_pairs_with_amp ä¼šä¸ºæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆæ–°çš„é…å¯¹
        else:
            print(f"\nç”Ÿæˆæµ‹è¯•é›†æ··åˆä¿¡å·å¯¹...")
            assert len(train_slices) == 0 or config['mode'] == 'both', "æµ‹è¯•é›†æ¨¡å¼ä¸‹ä¸åº”æœ‰è®­ç»ƒé›†åˆ‡ç‰‡"
            test_pairs_list = generate_pairs_from_slices(
                [], test_slices,  # ä¸¥æ ¼éš”ç¦»ï¼šåªä¼ å…¥æµ‹è¯•é›†åˆ‡ç‰‡
                target_pairs=test_target_pairs,
                amp_range=tuple(amp_range) if amp_range is not None else (0.2, 0.9), 
                seed=config['random_seed'] + 1
            )
            test_pairs = [(s1, i1, s2, i2, amp) for s1, i1, s2, i2, amp in test_pairs_list 
                          if s1 == 'test' and s2 == 'test']
            if len(test_pairs) != len(test_pairs_list):
                raise RuntimeError(f"æ•°æ®æ³„éœ²æ£€æµ‹: å‘ç°éæµ‹è¯•é›†é…å¯¹ï¼æµ‹è¯•é›†é…å¯¹ {len(test_pairs)} != æ€»é…å¯¹ {len(test_pairs_list)}")
            print(f"  æµ‹è¯•é›†é…å¯¹: {len(test_pairs)} ç»„ï¼ˆå·²éªŒè¯ï¼šå…¨éƒ¨ä¸ºæµ‹è¯•é›†å†…éƒ¨é…å¯¹ï¼‰")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(config['output_dir'])
    output_dir.mkdir(parents=True, exist_ok=True)
    
    train_output_dir = None
    test_output_dir = None
    
    if config['mode'] in ['train', 'both']:
        if config.get('train_demodulable', False):
            train_output_dir = output_dir / "train" / "demodulable"
        else:
            train_output_dir = output_dir / "train"
        train_output_dir.mkdir(parents=True, exist_ok=True)
    
    if config['mode'] in ['test', 'both']:
        add_noise_test = config.get('add_noise_to_target_snr', False) and config.get('test_demodulable', False)
        if not add_noise_test:
            test_output_dir = output_dir / "nonoise"
        elif config.get('test_demodulable', False):
            test_output_dir = output_dir / "test" / "demodulable"
        elif config.get('undemodulable', False):
            test_output_dir = output_dir / "test" / "undemodulable"
        else:
            test_output_dir = output_dir / "test"
        test_output_dir.mkdir(parents=True, exist_ok=True)
    
    # å¤„ç†è®­ç»ƒé›†é…å¯¹
    train_saved_paths = []
    if train_pairs:
        print(f"\nå¤„ç†è®­ç»ƒé›†é…å¯¹ï¼ˆæ¯ä¸ªshard {config['shard_size']} ç»„ï¼‰...")
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦æ·»åŠ å™ªå£°ï¼ˆä»…å¯¹å¯è§£è°ƒè®­ç»ƒæ•°æ®ï¼‰
        add_noise = config.get('add_noise_to_target_snr', False) and config.get('train_demodulable', False)
        if add_noise:
            print(f"  å°†è¯„ä¼°ä¸¤è·¯æºä¿¡å·å’Œæ··åˆä¿¡å·SNRï¼Œå¦‚æœæ··åˆä¿¡å·SNR>{config['target_snr_db']}dBï¼Œåˆ™æ·»åŠ å™ªå£°è‡³{config['target_snr_db']}dB")
            print(f"  å¦‚æœæ··åˆä¿¡å·SNR<{config['target_snr_db']}dBï¼Œåˆ™ä¸¢å¼ƒè¯¥é…å¯¹")
            print(f"  æ»¤æ³¢å™¨ç±»å‹ï¼š{config.get('filter_type', 'RRC')}")
        
        print(f"  ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†ï¼ˆ{config['num_workers']} ä¸ªå·¥ä½œè¿›ç¨‹ï¼‰...")
        
        # å‡†å¤‡å¤šè¿›ç¨‹å‚æ•°
        worker_args = []
        for pair_idx, (source1, idx1, source2, idx2, amp_ratio) in enumerate(train_pairs):
            sig1 = train_slices[idx1]
            sig2 = train_slices[idx2]
            noise_seed = config['random_seed'] + pair_idx if add_noise else None
            worker_args.append((
                sig1, sig2, amp_ratio, config['modulation'], config['sps'],
                source1, idx1, source2, idx2,
                add_noise, config['target_snr_db'], config.get('filter_type', 'RRC'), noise_seed
            ))
        
        # ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†
        print(f"  å¼€å§‹å¹¶è¡Œå¤„ç† {len(train_pairs)} ä¸ªé…å¯¹...")
        start_time = time.time()
        
        train_entries = []
        discarded_count = 0  # ç»Ÿè®¡ä¸¢å¼ƒçš„æ•°é‡
        with Pool(processes=config.get('num_workers', cpu_count())) as pool:
            # ä½¿ç”¨imapä»¥ä¾¿æ˜¾ç¤ºè¿›åº¦
            results = pool.imap(_create_entry_worker, worker_args)
            
            # æ˜¾ç¤ºè¿›åº¦
            completed = 0
            total = len(worker_args)
            last_print_time = time.time()
            print_interval = 2.0  # æ¯2ç§’æ‰“å°ä¸€æ¬¡è¿›åº¦
            
            for result in results:
                completed += 1
                
                # è¿‡æ»¤æ‰Noneå€¼ï¼ˆSNRä½äºç›®æ ‡å€¼çš„è¢«ä¸¢å¼ƒï¼‰
                if result is None:
                    discarded_count += 1
                else:
                    train_entries.append(result)
                
                # å®šæœŸæ‰“å°è¿›åº¦
                current_time = time.time()
                if current_time - last_print_time >= print_interval or completed == total:
                    elapsed = current_time - start_time
                    rate = completed / elapsed if elapsed > 0 else 0
                    remaining = (total - completed) / rate if rate > 0 else 0
                    progress_pct = completed / total * 100
                    valid_count = len(train_entries)
                    print(f"    è¿›åº¦: {completed}/{total} ({progress_pct:.1f}%) | "
                          f"æœ‰æ•ˆ: {valid_count} | ä¸¢å¼ƒ: {discarded_count} | "
                          f"å·²ç”¨: {elapsed:.1f}s | é€Ÿåº¦: {rate:.1f} é…å¯¹/s | "
                          f"é¢„è®¡å‰©ä½™: {remaining:.1f}s", flush=True)
                    last_print_time = current_time
        
        elapsed_time = time.time() - start_time
        print(f"  å¹¶è¡Œå¤„ç†å®Œæˆï¼Œè€—æ—¶ {elapsed_time:.2f} ç§’ï¼ˆå¹³å‡ {elapsed_time/len(train_pairs)*1000:.2f} ms/é…å¯¹ï¼‰")
        print(f"  æœ‰æ•ˆé…å¯¹: {len(train_entries)} (å…¨éƒ¨ä¿ç•™)")
        
        # ç»Ÿè®¡SNRä¿¡æ¯
        snr_stats = []  # ç»Ÿè®¡æ··åˆä¿¡å·SNRä¿¡æ¯
        noise_added_count = 0
        
        train_shard_entries = []
        train_shard_idx = 0
        
        for entry in train_entries:
            train_shard_entries.append(entry)
            
            # ç»Ÿè®¡SNRä¿¡æ¯ï¼ˆä»entryä¸­æå–ï¼‰
            if add_noise:
                actual_snr = entry['params'][0]
                snr_stats.append(actual_snr)
                
                if actual_snr <= config['target_snr_db'] + 0.1:  # å…è®¸0.1dBè¯¯å·®
                    noise_added_count += 1
            
            # å¦‚æœè¾¾åˆ°shardå¤§å°ï¼Œä¿å­˜
            if len(train_shard_entries) >= config['shard_size']:
                total_shards = (len(train_pairs) + config['shard_size'] - 1) // config['shard_size']
                save_path = save_shard(train_shard_entries, train_shard_idx, train_output_dir, 
                                      config['modulation'], total_shards, tuple(amp_range) if amp_range is not None else (0.2, 0.9))
                if save_path:
                    train_saved_paths.append(save_path)
                train_shard_entries = []
                train_shard_idx += 1
        
        # ä¿å­˜æœ€åä¸€ä¸ªshard
        if train_shard_entries:
            total_shards = train_shard_idx + 1
            save_path = save_shard(train_shard_entries, train_shard_idx, train_output_dir, 
                                  config['modulation'], total_shards, tuple(amp_range) if amp_range is not None else (0.2, 0.9))
            if save_path:
                train_saved_paths.append(save_path)
        
        # è¾“å‡ºSNRç»Ÿè®¡ä¿¡æ¯
        if add_noise and snr_stats:
            print(f"\n  SNRç»Ÿè®¡ä¿¡æ¯:")
            print(f"    æœ€ç»ˆæ··åˆä¿¡å·å¹³å‡SNR: {np.mean(snr_stats):.2f} dB")
            print(f"    æœ€ç»ˆæ··åˆä¿¡å·æœ€å°SNR: {np.min(snr_stats):.2f} dB")
            print(f"    æœ€ç»ˆæ··åˆä¿¡å·æœ€å¤§SNR: {np.max(snr_stats):.2f} dB")
            print(f"    å·²æ·»åŠ å™ªå£°æ ·æœ¬æ•°: {noise_added_count}/{len(train_pairs)}")

    
    # å¤„ç†æµ‹è¯•é›†é…å¯¹
    test_saved_paths = []
    
    # å¦‚æœä½¿ç”¨ num_files_per_amp ä¸”æ²¡æœ‰ä½¿ç”¨ amp_listï¼ˆtest1_2 æ¨¡å¼ï¼‰
    if config.get('num_files_per_amp', 0) > 0 and (amp_list is None or len(amp_list) == 0):
        # test1_2 æ¨¡å¼ï¼šå•ä¸ªå¹…åº¦æ¯”ï¼Œç”Ÿæˆå¤šä¸ªæ–‡ä»¶
        if isinstance(amp_range, tuple):
            amp_ratio = amp_range[0] if amp_range[0] == amp_range[1] else amp_range[0]
        else:
            amp_ratio = amp_range[0] if isinstance(amp_range, list) and len(amp_range) > 0 else 0.7
        print(f"\n{'='*60}")
        print(f"å¤„ç† test1_2 æ¨¡å¼ï¼ˆå¹…åº¦æ¯”={amp_ratio}ï¼Œæ¯ä¸ªæ–‡ä»¶ {config.get('samples_per_file', 30)} ä¸ªæ ·æœ¬ï¼Œç”Ÿæˆ {config['num_files_per_amp']} ä¸ªæ–‡ä»¶ï¼‰")
        print(f"{'='*60}")
        
        # ç›´æ¥è°ƒç”¨ process_test_pairs_with_ampï¼Œä¼ å…¥ç©ºçš„ test_pairsï¼ˆå‡½æ•°å†…éƒ¨ä¼šä¸ºæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆé…å¯¹ï¼‰
        test_saved_paths = process_test_pairs_with_amp(
            [], test_slices, config, amp_ratio
        )
    
    # å¸¸è§„æ¨¡å¼ï¼ˆå¦‚æœæ²¡æœ‰ä½¿ç”¨amp_listï¼Œä¸”æ²¡æœ‰ä½¿ç”¨num_files_per_ampï¼‰
    elif test_pairs and not (amp_list is not None and len(amp_list) > 0) and config.get('num_files_per_amp', 0) == 0:
        print(f"\nå¤„ç†æµ‹è¯•é›†é…å¯¹ï¼ˆæ¯ä¸ªshard {config['shard_size']} ç»„ï¼‰...")
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦æ·»åŠ å™ªå£°ï¼ˆä»…å¯¹å¯è§£è°ƒæµ‹è¯•æ•°æ®ï¼Œä¸”æ˜ç¡®æŒ‡å®šäº†add_noise_to_target_snrï¼‰
        add_noise_test = config.get('add_noise_to_target_snr', False) and config.get('test_demodulable', False)
        if add_noise_test:
            print(f"  å°†å‡è®¾ä¿¡å·1å’Œä¿¡å·2æ— å™ªï¼Œç›´æ¥å¯¹åˆè·¯ä¿¡å·åŠ å™ªè‡³{config['target_snr_db']}dB")
        else:
            print(f"  ç›´æ¥ç»„åˆæ•°æ®ï¼Œä¸æ·»åŠ å™ªå£°")
        
        print(f"  ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†ï¼ˆ{config['num_workers']} ä¸ªå·¥ä½œè¿›ç¨‹ï¼‰...")
        
        # å‡†å¤‡å¤šè¿›ç¨‹å‚æ•°
        worker_args = []
        for pair_idx, (source1, idx1, source2, idx2, amp_ratio) in enumerate(test_pairs):
            sig1 = test_slices[idx1]
            sig2 = test_slices[idx2]
            noise_seed = config['random_seed'] + pair_idx + len(train_pairs) if add_noise_test else None
            worker_args.append((
                sig1, sig2, amp_ratio, config['modulation'], config['sps'],
                source1, idx1, source2, idx2,
                add_noise_test, config['target_snr_db'], config.get('filter_type', 'RRC'), noise_seed
            ))
        
        # ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†
        print(f"  å¼€å§‹å¹¶è¡Œå¤„ç† {len(test_pairs)} ä¸ªé…å¯¹...")
        start_time = time.time()
        
        test_entries = []
        discarded_count = 0  # ç»Ÿè®¡ä¸¢å¼ƒçš„æ•°é‡
        with Pool(processes=config.get('num_workers', cpu_count())) as pool:
            # ä½¿ç”¨imapä»¥ä¾¿æ˜¾ç¤ºè¿›åº¦
            results = pool.imap(_create_entry_worker, worker_args)
            
            # æ˜¾ç¤ºè¿›åº¦
            completed = 0
            total = len(worker_args)
            last_print_time = time.time()
            print_interval = 2.0  # æ¯2ç§’æ‰“å°ä¸€æ¬¡è¿›åº¦
            
            for result in results:
                completed += 1
                
                # è¿‡æ»¤æ‰Noneå€¼ï¼ˆSNRä½äºç›®æ ‡å€¼çš„è¢«ä¸¢å¼ƒï¼‰
                if result is None:
                    discarded_count += 1
                else:
                    test_entries.append(result)
                
                # å®šæœŸæ‰“å°è¿›åº¦
                current_time = time.time()
                if current_time - last_print_time >= print_interval or completed == total:
                    elapsed = current_time - start_time
                    rate = completed / elapsed if elapsed > 0 else 0
                    remaining = (total - completed) / rate if rate > 0 else 0
                    progress_pct = completed / total * 100
                    valid_count = len(test_entries)
                    print(f"    è¿›åº¦: {completed}/{total} ({progress_pct:.1f}%) | "
                          f"æœ‰æ•ˆ: {valid_count} | ä¸¢å¼ƒ: {discarded_count} | "
                          f"å·²ç”¨: {elapsed:.1f}s | é€Ÿåº¦: {rate:.1f} é…å¯¹/s | "
                          f"é¢„è®¡å‰©ä½™: {remaining:.1f}s", flush=True)
                    last_print_time = current_time
        
        elapsed_time = time.time() - start_time
        print(f"  å¹¶è¡Œå¤„ç†å®Œæˆï¼Œè€—æ—¶ {elapsed_time:.2f} ç§’ï¼ˆå¹³å‡ {elapsed_time/len(test_pairs)*1000:.2f} ms/é…å¯¹ï¼‰")
        if add_noise_test:
            print(f"  æœ‰æ•ˆé…å¯¹: {len(test_entries)} (å…¨éƒ¨ä¿ç•™)")
        else:
            print(f"  æœ‰æ•ˆé…å¯¹: {len(test_entries)} (å…¨éƒ¨ä¿ç•™ï¼Œæœªè¯„ä¼°SNR)")
        
        # ç»Ÿè®¡SNRä¿¡æ¯
        snr_stats = []  # ç»Ÿè®¡æ··åˆä¿¡å·SNRä¿¡æ¯
        noise_added_count = 0
        
        test_shard_entries = []
        test_shard_idx = 0
        
        for entry in test_entries:
            test_shard_entries.append(entry)
            
            # ç»Ÿè®¡SNRä¿¡æ¯ï¼ˆä»entryä¸­æå–ï¼‰
            if add_noise_test:
                actual_snr = entry['params'][0]
                snr_stats.append(actual_snr)
                
                if actual_snr <= config['target_snr_db'] + 0.1:  # å…è®¸0.1dBè¯¯å·®
                    noise_added_count += 1
            
            # å¦‚æœè¾¾åˆ°shardå¤§å°ï¼Œä¿å­˜
            if len(test_shard_entries) >= config['shard_size']:
                total_shards = (len(test_pairs) + config['shard_size'] - 1) // config['shard_size']
                save_path = save_shard(test_shard_entries, test_shard_idx, test_output_dir, 
                                      config['modulation'], total_shards, tuple(amp_range) if amp_range is not None else (0.2, 0.9))
                if save_path:
                    test_saved_paths.append(save_path)
                test_shard_entries = []
                test_shard_idx += 1
        
        # ä¿å­˜æœ€åä¸€ä¸ªshard
        if test_shard_entries:
            total_shards = test_shard_idx + 1
            save_path = save_shard(test_shard_entries, test_shard_idx, test_output_dir, 
                                  config['modulation'], total_shards, tuple(amp_range) if amp_range is not None else (0.2, 0.9))
            if save_path:
                test_saved_paths.append(save_path)
        
        # è¾“å‡ºSNRç»Ÿè®¡ä¿¡æ¯
        if add_noise_test and snr_stats:
            print(f"\n  SNRç»Ÿè®¡ä¿¡æ¯:")
            print(f"    æœ€ç»ˆæ··åˆä¿¡å·å¹³å‡SNR: {np.mean(snr_stats):.2f} dB")
            print(f"    æœ€ç»ˆæ··åˆä¿¡å·æœ€å°SNR: {np.min(snr_stats):.2f} dB")
            print(f"    æœ€ç»ˆæ··åˆä¿¡å·æœ€å¤§SNR: {np.max(snr_stats):.2f} dB")
            print(f"    å·²æ·»åŠ å™ªå£°æ ·æœ¬æ•°: {noise_added_count}/{len(test_pairs)}")
    
    # ä¿å­˜å…ƒæ•°æ®
    metadata = {
        'modulation': config['modulation'],
        'mode': config['mode'],
        'train_slices_file': str(train_slices_path) if train_slices_path else None,
        'test_slices_file': str(test_slices_path) if test_slices_path else None,
        'train_pairs': len(train_pairs),
        'test_pairs': len(test_pairs) if 'test_pairs' in locals() else 0,
        'shard_size': config['shard_size'],
        'train_shards': len(train_saved_paths),
        'test_shards': len(test_saved_paths),
        'amp_range': list(amp_range) if amp_range is not None else None,
        'amp_list': amp_list if amp_list is not None else None,
        'sps': config['sps'],
        'seed': config['random_seed'],
        'train_demodulable': config.get('train_demodulable', False),
        'test_demodulable': config.get('test_demodulable', False),
        'test_undemodulable': config.get('undemodulable', False),
        'add_noise_to_target_snr': config.get('add_noise_to_target_snr', False),
        'target_snr_db': config.get('target_snr_db') if config.get('add_noise_to_target_snr', False) else None,
        'filter_type': config.get('filter_type') if config.get('add_noise_to_target_snr', False) else None,
        'generated_at': str(datetime.now()),
        'train_shard_files': [str(p) for p in train_saved_paths],
        'test_shard_files': [str(p) for p in test_saved_paths]
    }
    
    # åœ¨metadataæ–‡ä»¶åä¸­åŒ…å«amp_rangeä¿¡æ¯ï¼Œé¿å…è¦†ç›–
    if amp_range is not None:
        amp_min, amp_max = amp_range
        metadata_filename = f"metadata_amp{amp_min:.1f}to{amp_max:.1f}.json"
    elif amp_list is not None:
        amp_str = "_".join([f"{a:.1f}" for a in amp_list])
        metadata_filename = f"metadata_ampList_{amp_str}.json"
    else:
        metadata_filename = "metadata.json"
    metadata_path = output_dir / metadata_filename
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    print(f"\nä¿å­˜å…ƒæ•°æ®: {metadata_path}")
    
    print(f"\n{'='*60}")
    print(f"å®Œæˆï¼")
    if train_pairs:
        print(f"  è®­ç»ƒé›†é…å¯¹: {len(train_pairs)} ç»„ï¼Œä¿å­˜äº† {len(train_saved_paths)} ä¸ªshardæ–‡ä»¶")
        print(f"  è®­ç»ƒé›†ç›®å½•: {train_output_dir}")
        if config.get('train_demodulable', False):
            print(f"  ï¼ˆå¯è§£è°ƒæ•°æ®æ¨¡å¼ï¼‰")
    if 'test_pairs' in locals() and test_pairs:
        print(f"  æµ‹è¯•é›†é…å¯¹: {len(test_pairs)} ç»„ï¼Œä¿å­˜äº† {len(test_saved_paths)} ä¸ªshardæ–‡ä»¶")
        print(f"  æµ‹è¯•é›†ç›®å½•: {test_output_dir}")
        if config.get('test_demodulable', False):
            print(f"  ï¼ˆå¯è§£è°ƒæ•°æ®æ¨¡å¼ï¼‰")
        elif config.get('undemodulable', False):
            print(f"  ï¼ˆä¸å¯è§£è°ƒæ•°æ®æ¨¡å¼ï¼‰")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"æ¨¡å¼: {config['mode']}ï¼ˆä¸¥æ ¼éš”ç¦»ï¼š{'è®­ç»ƒé›†å’Œæµ‹è¯•é›†' if config['mode'] == 'both' else 'ä»…' + ('è®­ç»ƒé›†' if config['mode'] == 'train' else 'æµ‹è¯•é›†')}ï¼‰")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()

